{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        ## CS 230 PROJECT BY ELLEN ROPER (eroper) AND TRISTAN GOSAKTI (tgosakti) ##\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "strokeTypes = 5\n",
    "fullSize = 2000\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PARSING ###\n",
    "trainSamples = []\n",
    "trainSamplesCNN = []\n",
    "groundTruth = []\n",
    "noisy = []\n",
    "noisycnn = []\n",
    "noisytruth = []\n",
    "noisytruthcnn = []\n",
    "groundtruthcnn = []\n",
    "\n",
    "for strokeType in range (1, strokeTypes+1):\n",
    "    noFiles = 10\n",
    "    for i in range (0, noFiles): #processing every file/batch #FIX LATER\n",
    "\n",
    "        #naming\n",
    "        frameDataName = str(strokeType) + \"-\" + str(i+1) + \"-TSFrameData.txt\"\n",
    "        motionDataName = str(strokeType) + \"-\" + str(i+1) + \"-TSMotionData.txt\"\n",
    "\n",
    "        #loading and processing\n",
    "        frameData = np.loadtxt(frameDataName)\n",
    "            \n",
    "        with open(motionDataName) as f: #for every file\n",
    "            for line in f: #for every json entry, should be around 10 \n",
    "                line = json.loads(line)\n",
    "                oneSample = []\n",
    "                for i in range (int(line[\"startIndex\"]), int(line[\"endIndex\"])):\n",
    "                    \n",
    "                    \n",
    "                    timeSlice = frameData[i]\n",
    "                    timeSlice = np.delete(timeSlice[1:], slice(None, None, 4))\n",
    "\n",
    "                    oneSample.append(timeSlice)\n",
    "                oneSampleNp = np.vstack(oneSample)\n",
    "                oneSampleFlatten = oneSampleNp.flatten()\n",
    "                noise = np.random.normal(0.0, 1, oneSampleNp.shape)\n",
    "                flattenNoise = noise.flatten()\n",
    "                oneSampleFlattenup = oneSampleFlatten + flattenNoise\n",
    "                oneSampleFlattendown = oneSampleFlatten - flattenNoise\n",
    "\n",
    "                \n",
    "                #TRAIN DATA FOR CNN\n",
    "                CNNSample = np.transpose(oneSampleNp)\n",
    "                CNNSampleup = CNNSample+np.transpose(noise)\n",
    "                CNNSampledown = CNNSample-np.transpose(noise)\n",
    "                CNNSamplepadded = np.pad(CNNSample, [(0,0), (0,111-CNNSample.shape[1])], 'constant')\n",
    "                CNNSampleuppadded = np.pad(CNNSampleup,[(0,0), (0,111-CNNSampleup.shape[1])], 'constant')\n",
    "                CNNSampledownpadded = np.pad(CNNSampledown, [(0,0), (0,111-CNNSampledown.shape[1])], 'constant')\n",
    "                trainSamplesCNN.append(CNNSamplepadded)\n",
    "                noisycnn.append(CNNSampleuppadded)\n",
    "                noisycnn.append(CNNSampledownpadded)\n",
    "                \n",
    "                #TRAIN DATA FOR OTHER MODELS\n",
    "                oneSamplePadded = np.pad(oneSampleFlatten, (0, fullSize-oneSampleFlatten.size), 'constant')             \n",
    "                oneSampleFlattenupPadded = np.pad(oneSampleFlattenup, (0, fullSize-oneSampleFlattenup.size), 'constant')\n",
    "                oneSampleFlattendownPadded = np.pad(oneSampleFlattendown, (0, fullSize-oneSampleFlattendown.size), 'constant')\n",
    "                noisy.append(oneSampleFlattenupPadded)\n",
    "                noisy.append(oneSampleFlattendownPadded)\n",
    "                trainSamples.append(oneSamplePadded)\n",
    "                \n",
    "                \n",
    "                #making y for every x\n",
    "                singleY = [0] * strokeTypes\n",
    "                singleY[strokeType-1] = 1\n",
    "                groundTruth.append(singleY)\n",
    "                groundtruthcnn.append(singleY)\n",
    "                noisytruthcnn.append(singleY)\n",
    "                noisytruthcnn.append(singleY)\n",
    "                noisytruth.append(singleY)\n",
    "                noisytruth.append(singleY)\n",
    "\n",
    "x_train_cnn1 = np.asarray(trainSamplesCNN)\n",
    "y_train_cnn1 = np.asarray(groundtruthcnn)\n",
    "\n",
    "noisy_train_cnn = np.asarray(noisycnn)\n",
    "noisy_train_y_cnn = np.asarray(noisytruthcnn)\n",
    "\n",
    "trainSamplesnp = np.vstack(trainSamples)\n",
    "groundTruthnp = np.vstack(groundTruth)\n",
    "noisySamplesnp = np.vstack(noisy)\n",
    "noisytruthnp = np.vstack(noisytruth)\n",
    "\n",
    "x_train1 = np.transpose(trainSamplesnp)\n",
    "y_train1 = np.transpose(groundTruthnp)\n",
    "noisy_train = np.transpose(noisySamplesnp)\n",
    "noisy_train_y = np.transpose(noisytruthnp)\n",
    "    \n",
    "shuffle_in_unison(np.transpose(x_train1), np.transpose(y_train1))\n",
    "shuffle_in_unison(x_train_cnn1, y_train_cnn1)\n",
    "\n",
    "x_train_cnn = x_train_cnn1[0:407]\n",
    "y_train_cnn = y_train_cnn1[0:407]\n",
    "x_dev_cnn = x_train_cnn1[407:457]\n",
    "y_dev_cnn = y_train_cnn1[407:457]\n",
    "x_test_cnn = x_train_cnn1[457:507]\n",
    "y_test_cnn = y_train_cnn1[457:507]\n",
    "\n",
    "x_dev_cnn = np.expand_dims(x_dev_cnn, axis=3)\n",
    "x_test_cnn = np.expand_dims(x_test_cnn, axis=3)\n",
    "\n",
    "alltrain = []\n",
    "alltrainy = []\n",
    "for k in range(0,407):\n",
    "    alltrain.append(x_train_cnn[k])\n",
    "    alltrainy.append(y_train_cnn[k])\n",
    "for l in range(0, 1014):\n",
    "    alltrain.append(noisy_train_cnn[l])\n",
    "    alltrainy.append(noisy_train_y_cnn[l])\n",
    "what = np.asarray(alltrain)\n",
    "y_train_cnn = np.asarray(alltrainy)\n",
    "x_train_cnn = np.expand_dims(what, axis=3)\n",
    "\n",
    "x_trainT = np.transpose(x_train1)\n",
    "y_trainT = np.transpose(y_train1)\n",
    "\n",
    "x_train = np.transpose(x_trainT[:407])\n",
    "y_train = np.transpose(y_trainT[:407])\n",
    "x_dev = np.transpose(x_trainT[407:457])\n",
    "y_dev = np.transpose(y_trainT[407:457])\n",
    "x_test = np.transpose(x_trainT[457:507])\n",
    "y_test = np.transpose(y_trainT[457:507])\n",
    "\n",
    "x_train = np.concatenate((x_train, noisy_train), axis=1)\n",
    "y_train = np.concatenate((y_train, noisy_train_y), axis=1)\n",
    "shuffle_in_unison(np.transpose(x_train), np.transpose(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN ###\n",
    "def create_placeholders(height_input, width_input, num_channels, num_strokes):\n",
    "    X = tf.placeholder(tf.float32, [None, height_input, width_input, num_channels])\n",
    "    Y = tf.placeholder(tf.float32, [None, num_strokes])\n",
    "    return X,Y\n",
    "\n",
    "def initialize_parameters():\n",
    "    tf.set_random_seed(1)\n",
    "    w1 = tf.get_variable(\"w1\", [3,3,1,10], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    w2 = tf.get_variable(\"w2\", [6,5,10,10], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    w3 = tf.get_variable(\"w3\", [1,5,10,1], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    parameters = {\"w1\": w1, \"w2\": w2, \"w3\": w3}\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    w1 = parameters['w1']\n",
    "    w2 = parameters['w2']\n",
    "    w3 = parameters['w3']\n",
    "    \n",
    "    Z1 = tf.nn.conv2d(X, w1, strides = [1,3,1,1], padding = 'VALID') \n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.nn.conv2d(A1, w2, strides = [1,1,1,1], padding='VALID')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,1,2,1], strides = [1,1,2,1], padding = 'VALID')\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    Z4 = tf.contrib.layers.fully_connected(P2, 5, activation_fn =tf.nn.softmax)\n",
    "    return Z4\n",
    "\n",
    "def compute_cost(Z4, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z4, labels=Y))\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN Model ###\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(0)\n",
    "(m, height, width, num_channels) = x_train_cnn.shape\n",
    "n_y = y_train_cnn.shape[1]\n",
    "costs = []\n",
    "learning_rate = 0.001\n",
    "X,Y = create_placeholders(height,width,1,n_y)\n",
    "parameters = initialize_parameters()\n",
    "Z4 = forward_propagation(X, parameters)\n",
    "cost = compute_cost(Z4, Y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "num_batches = m;\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(55):\n",
    "        sess.run([optimizer,cost], feed_dict={X: x_train_cnn, Y: y_train_cnn})\n",
    "        \n",
    "    parameters = sess.run(parameters)\n",
    "    correct_prediction = tf.equal(tf.argmax(Z4,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train Accuracy:\", accuracy.eval({X: x_train_cnn, Y: y_train_cnn}))\n",
    "    print(\"Dev Accuracy: \", accuracy.eval({X: x_dev_cnn, Y: y_dev_cnn}))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: x_test_cnn, Y: y_test_cnn}))\n",
    "    labels = tf.argmax(Y, axis = 1)\n",
    "    predicted = tf.argmax(Z4, axis = 1)\n",
    "    confusion_matrix = tf.confusion_matrix(labels, predicted, 5) #5 strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEURAL NETWORK IMPLEMENTATION: 1 hidden layer ###\n",
    "#create placeholders: \n",
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, [n_x, None], name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None], name = \"Y\")\n",
    "    return X,Y\n",
    "\n",
    "#initialize weight and biases\n",
    "def initialize_parameters(n_x, n_y, n_hidden_1):\n",
    "    w1 = tf.get_variable(\"w1\", shape=(n_hidden_1, n_x), initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    w2 = tf.get_variable(\"w2\", shape=(n_y, n_hidden_1), initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    b1 = tf.get_variable(\"b1\", [n_hidden_1,1], initializer = tf.zeros_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", [num_classes,1], initializer=tf.zeros_initializer())\n",
    "    parameters = {\"W1\": w1, \"W2\": w2, \"b1\": b1, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "\n",
    "#create the model\n",
    "def forward_propagation(x, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    layer_1 = tf.matmul(W1,x) + parameters['b1']               \n",
    "    out_layer = tf.matmul(parameters['W2'], layer_1) + parameters['b2']\n",
    "    return out_layer\n",
    "\n",
    "def compute_cost(Z, Y):\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 Hidden Layer ###\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(0)\n",
    "X_train = x_train\n",
    "Y_train = y_train\n",
    "X_test = x_test\n",
    "Y_test = y_test\n",
    "learning_rate = 0.1\n",
    "n_hidden_1 = 500\n",
    "num_classes = strokeTypes\n",
    "(n_x, m) = X_train.shape\n",
    "n_y = Y_train.shape[0]\n",
    "X,Y = create_placeholders(n_x, n_y)\n",
    "parameters = initialize_parameters(n_x, n_y, n_hidden_1)\n",
    "Z = forward_propagation(X, parameters) \n",
    "cost = compute_cost(Z, Y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(10):\n",
    "        sess.run([optimizer, cost], feed_dict={X: x_train, Y: y_train})\n",
    "    parameters = sess.run(parameters)\n",
    "    correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train Accuracy:\", accuracy.eval({X: x_train, Y: y_train}))\n",
    "    print(\"Dev accuracy: \", accuracy.eval({X: x_dev, Y: y_dev}))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEEP NETWORK IMPLEMENTATION ###\n",
    "def create_placeholders_deep(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x,None), name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y,None), name=\"Y\")\n",
    "    return X,Y\n",
    "\n",
    "def initialize_parameters_deep(unitsvector, n_x):    \n",
    "    w1 = tf.get_variable(\"w1\", shape=(unitsvector['n1'], n_x), initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    b1 = tf.get_variable(\"b1\", [unitsvector['n1'], 1], initializer=tf.zeros_initializer())\n",
    "    w2 = tf.get_variable(\"w2\", shape=(unitsvector['n2'],unitsvector['n1']), initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    b2 = tf.get_variable(\"b2\", [unitsvector['n2'],1], initializer=tf.zeros_initializer())\n",
    "    w3 = tf.get_variable(\"w3\", shape=(unitsvector['n3'], unitsvector['n2']), initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    b3 = tf.get_variable(\"b3\", [unitsvector['n3'], 1], initializer=tf.zeros_initializer())\n",
    "    w4 = tf.get_variable(\"w4\", shape = (unitsvector['n4'], unitsvector['n3']), initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    b4 = tf.get_variable(\"b4\", [unitsvector['n4'], 1], initializer=tf.zeros_initializer())\n",
    "    parameters = {\"w1\":w1, \"w2\":w2, \"w3\": w3, \"w4\": w4, \"b1\":b1, \"b2\":b2, \"b3\": b3, \"b4\": b4}\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation_deep(X, parameters):\n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    w3 = parameters['w3']\n",
    "    b3 = parameters['b3']\n",
    "    w4 = parameters['w4']\n",
    "    b4 = parameters['b4']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(w1,X), b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(w2,A1), b2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(w3, A2), b3)\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    Z4 = tf.add(tf.matmul(w4, A3), b4)\n",
    "    \n",
    "    return Z4\n",
    "\n",
    "def compute_cost_deep(Z4, Y):\n",
    "    logits = tf.transpose(Z4)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### DEEP NETWORK ###\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(0)\n",
    "X_train = x_train\n",
    "Y_train= y_train\n",
    "X_test = x_test\n",
    "Y_test = y_test\n",
    "learning_rate = 0.01\n",
    "num1 = 550\n",
    "num2 = 300\n",
    "num3 = 100\n",
    "num4 = 5\n",
    "num_classes = 5\n",
    "(n_x, m) = X_train.shape\n",
    "n_y = Y_train.shape[0]\n",
    "costs = []\n",
    "X,Y = create_placeholders_deep(n_x,n_y)\n",
    "unitsvector = {\"n1\": num1, \"n2\": num2, \"n3\": num3, \"n4\": num4}\n",
    "parameters = initialize_parameters_deep(unitsvector, n_x)\n",
    "Z4 = forward_propagation_deep(X, parameters)\n",
    "cost = compute_cost_deep(Z4, Y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(20):\n",
    "        sess.run([optimizer,cost], feed_dict={X: X_train, Y:Y_train})\n",
    "    parameters = sess.run(parameters)\n",
    "    correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train Accuracy: \", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "    print(\"Dev Accuracy: \", accuracy.eval({X: x_dev, Y: y_dev}))\n",
    "    print(\"Test Accuracy: \", accuracy.eval({X: X_test, Y: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
